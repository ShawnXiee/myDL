{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubConv(nn.Module):\n",
    "    # 2个3*3 Conv结构, no padding\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "# t1 = DoubConv(3, 64)\n",
    "# x = torch.randn(1, 3, 572, 572)\n",
    "# t1(x).shape\n",
    "# (1, 64, 568, 568)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.double_conv = DoubConv(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(self.max_pool(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        # feacher map翻倍，需要kernel=2， stride=2\n",
    "        # out = (in + 2* padding -kernel)/stride + 1\n",
    "        # in = (out -1)* stride + kernel - 2*padding\n",
    "        # in = (out - 1) * 2 + 2 = 2 * out\n",
    "        self.double_conv = DoubConv(in_channels, out_channels)\n",
    "        \n",
    "    # 根据up的size对bridge进行裁剪\n",
    "    def center_crop(self, up, bridge):\n",
    "        _,_,up_h,up_w = up.size()\n",
    "        _,_,bridge_h,bridge_w = up.size()\n",
    "        diff_h = bridge_h - up_h\n",
    "        diff_w = bridge_w - up_w\n",
    "        return bridge[:,:, diff_h//2:diff_h//2+up_h, diff_w //2:diff_w+up_w]\n",
    "    \n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop = self.center_crop(up, bridge)\n",
    "#         print('up', up.shape)\n",
    "#         print('crop', crop.shape)\n",
    "        # cat操作，增加channel，所以dim为1\n",
    "        out = torch.cat([up, crop], 1)\n",
    "        return self.double_conv(out)\n",
    "# x = torch.randn(1, 3, 572, 572)\n",
    "# m1 = DoubConv(3, 64)\n",
    "# Down1 = Down(64, 128)\n",
    "# Down2 = Down(128, 256)\n",
    "# Down3 = Down(256, 512)\n",
    "# Down4 = Down(512, 1024)\n",
    "# up4 = Up(1024, 512)\n",
    "# up3 = Up(512, 256)\n",
    "# up2 = Up(256, 128)\n",
    "# up1 = Up(128, 64)\n",
    "# d1 = m1(x)\n",
    "# d2 = Down1(d1)\n",
    "# d3 = Down2(d2)\n",
    "# d4 = Down3(d3)\n",
    "# x = Down4(d4)\n",
    "# u4 = up4(x, d4)\n",
    "# u3 = up3(u4, d3)\n",
    "# u2 = up2(u3, d2)\n",
    "# u1 = up1(u2, d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 388, 388])\n"
     ]
    }
   ],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,# 输入channels\n",
    "        n_class, # 输出class\n",
    "        depth, # unet深度 论文中为5\n",
    "        wf, # 第一层unet的channels 论文中为64\n",
    "    ):\n",
    "        super(Unet, self).__init__()\n",
    "        self.down_path = nn.ModuleList()\n",
    "        self.up_path = nn.ModuleList()\n",
    "#         self.double_conv = DoubConv(in_channels, wf)\n",
    "        self.out_conv = nn.Conv2d(wf, n_class, kernel_size=1)\n",
    "        self.down_path.append(DoubConv(in_channels, wf))\n",
    "        for i in range(depth-1):\n",
    "            self.down_path.append(Down(wf*2**i, wf*2**(i+1)))\n",
    "        for i in reversed(range(depth-1)):\n",
    "            self.up_path.append(Up(wf*2**(i+1), wf*2**i))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bridges = []\n",
    "#         x = self.double_conv(x)\n",
    "        down = x\n",
    "        for i in range(len(self.down_path)):\n",
    "#             print(self.down_path[i])\n",
    "            down = self.down_path[i](down)\n",
    "            if i != len(self.down_path) -1:\n",
    "                bridges.append(down)\n",
    "        up = down\n",
    "        for i in range(len(self.up_path)):\n",
    "            up = self.up_path[i](up, bridges[-(i+1)])\n",
    "        return self.out_conv(up)\n",
    "    \n",
    "unet = Unet(3, 2, 5, 64)\n",
    "x = torch.randn(1, 3, 572, 572)\n",
    "x = unet(x)\n",
    "print(x.shape)\n",
    "#torch.Size([1, 2, 388, 388])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoubConv(\n",
      "  (double_conv): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#双线性插值和装置卷积\n",
    "# 双线性插值用来代替转置卷积做上采样\n",
    "# 1. 1*1 kernel 的作用：\n",
    "# （1）调整channels\n",
    "# （2）增加特征提取能力\n",
    "# 2. 为啥fcn的padding=100\n",
    "# (i + 2p - k)/s + 1 <= 7\n",
    "# s = 32 k=3    6*32 + 3 = 195 所以p取100\n",
    "\n",
    "# 3.转置卷积恢复形状，不能恢复数值\n",
    "# 4.FCn的skip是做什么的\n",
    "# 特征融合\n",
    "# x = torch.randn(1,2,3,4)\n",
    "# torch.cat([x,x],1).shape\n",
    "# torch.cat?\n",
    "# torch.Size([1, 4, 3, 4])\n",
    "print(DoubConv(3, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-34-4198aef6a38b>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-4198aef6a38b>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    for dst_x in range(dst_w):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def Bi(src, new_size):\n",
    "    dst_h, dst_w = new_size\n",
    "    src_h, src_w = src[:2]\n",
    "    if des_h == src_h and des_w == src_w:\n",
    "        return src.copy()\n",
    "    scale_x = float(src_w) / des_w\n",
    "    scale_y = float(src_h) / des_h\n",
    "    dst = np.zeros((dst_h, dst_w, 3))\n",
    "    for i in range(3):\n",
    "        for dst_y in range(dst_h):\n",
    "            for dst_x in range(dst_w):\n",
    "                # 像素单位为1 每个像素（h，w）对应中心位置（h+0.5，w + 0.5）\n",
    "                src_x = (des_x + 0.5) * scale_x - 0.5\n",
    "                src_y = (des_y + 0.5) * scale_y - 0.5\n",
    "                # 左上角的点\n",
    "                src_x_0 = int(np.floor(src_x))\n",
    "                src_y_0 = int(np.floor(src_y))\n",
    "                #像素值边界为1，防止出界\n",
    "                src_x_1 = min(src_x_0 + 1, src_w -1)\n",
    "                src_x_1 = min(src_x_0 + 1, src_w -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-4d57dce00d1b>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-4d57dce00d1b>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    def UnetUpBlock(nn.Module):\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 下采样\n",
    "class UnetConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, padding, batch_norm):\n",
    "        super(UnetConvBlock, self).__init__()\n",
    "        block = [] #建造空列表\n",
    "        block.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU)\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_channels))\n",
    "        block.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU)\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_channels))\n",
    "        self.block = nn.Sequential(*block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "# 上采样\n",
    "def UnetUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_mode, padding, batch_norm):\n",
    "        super(UnetUpBlock, self).__init__()\n",
    "        if up_mode = \"upconv\":\n",
    "            self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        elif up_mode = \"upsample\":\n",
    "            self.up = nn.Sequential(nn.Upsample(mode='bilinera', scale_factor=2),\n",
    "                                    nn.Conv2d(in_channels, out_channels, kernel_size=1))\n",
    "        self.conv_block = UnetConvBlock(in_channels, out_channels, padding, batch_norm)\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _,_,layer_h,layer_w = layer.size()\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1, \n",
    "        n_class=2, \n",
    "        depth=5, \n",
    "        wf=6, \n",
    "        padding=False, \n",
    "        batch_norm=False, \n",
    "        up_mode='upconv'\n",
    "    ):\n",
    "        super(Unet, self).__init__()\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
